# ğŸ¦™ Llama.cpp Android Integration | Android å‘ã‘ Llama.cpp çµ±åˆ

A minimal, production-ready integration of [llama.cpp](https://github.com/ggerganov/llama.cpp) on Android using JNI and Jetpack Compose.

Jetpack Compose ã¨ JNI ã‚’ç”¨ã„ã¦ã€[llama.cpp](https://github.com/ggerganov/llama.cpp) ã‚’ Android ã«çµ±åˆã™ã‚‹æœ€å°é™ã‹ã¤å®Ÿç”¨çš„ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã™ã€‚

---

## âœ… Features / ä¸»ãªç‰¹å¾´

- Native model inference on-device (fully offline)  
  ãƒ¢ãƒ‡ãƒ«ã‚’ç«¯æœ«ä¸Šã§ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œï¼ˆå®Œå…¨ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å‹•ä½œï¼‰

- Built with NDK + CMake (`libllama.so`)  
  NDK + CMake ã«ã‚ˆã‚‹ C++ ãƒ“ãƒ«ãƒ‰ï¼ˆ`libllama.so`ï¼‰

- Kotlin-side wrapper with coroutine support  
  Kotlin ã‹ã‚‰ã‚³ãƒ«ãƒ¼ãƒãƒ³ã§å®‰å…¨ã«å‘¼ã³å‡ºã—

- Automatic CPU threading optimization  
  CPU æ§‹æˆã«å¿œã˜ãŸã‚¹ãƒ¬ãƒƒãƒ‰æœ€é©åŒ–

---

## ğŸš€ Quick Start / ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. Clone the repository / ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³

```bash
git clone git@github.com:ishizuki-tech/llama.git
cd llama
````

### 2. Add model to assets / ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ 

Place a quantized `.gguf` model (e.g., `Llama-3.2-1B-Instruct-Q4_K_M.gguf`) into:

```
app/src/main/assets/models/
```

> âš ï¸ GitHub ã§ã¯ 100MB è¶…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥ push ã§ãã¾ã›ã‚“ã€‚
> Use [Git LFS](https://git-lfs.github.com) or manage manually.
> Git LFS ã¾ãŸã¯æ‰‹å‹•ã§ç®¡ç†ã—ã¦ãã ã•ã„ã€‚

### 3. Open in Android Studio / Android Studio ã§é–‹ã

* Compile SDK: 35+
* NDK: 26.3+
* Run on device with **2GB+ RAM**
  ç«¯æœ«ã¯ RAM 2GB ä»¥ä¸Šæ¨å¥¨

---

## ğŸ“‚ Project Structure / ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ

```
.
â”œâ”€â”€ app/                        # Jetpack Compose UI
â”‚   â””â”€â”€ src/main/assets/models/ # ãƒ¢ãƒ‡ãƒ«æ ¼ç´ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
â”œâ”€â”€ nativelib/                 # JNI & ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚³ãƒ¼ãƒ‰
â”‚   â”œâ”€â”€ src/main/cpp/llama.cpp # llama.cpp ã‚½ãƒ¼ã‚¹
â”‚   â””â”€â”€ LlamaContext.kt        # JNI ãƒ©ãƒƒãƒ‘ãƒ¼ï¼ˆKotlinï¼‰
```

---

## ğŸ§  Sample Kotlin Usage / Kotlin åˆ©ç”¨ä¾‹

```kotlin
val llama = LlamaContext.init(context.assets, "models/Llama-3.2-1B-Instruct-Q4_K_M.gguf")

val response = llama.complete("What is the capital of Kenya?")

println(response) // => The capital of Kenya is Nairobi.
```

---

## ğŸ“¦ Model Suggestions / æ¨å¥¨ãƒ¢ãƒ‡ãƒ«

| Model                 | Size    | Min RAM | Notes                    |
| --------------------- | ------- | ------- | ------------------------ |
| LLaMA-3.2 1B Q4\_K\_M | \~770MB | \~2GB   | Good baseline for mobile |
| LLaMA-3.2 3B Q4\_K\_M | \~2.7GB | \~5GB+  | High-end Android only    |

---

## ğŸ”§ ABI Support / ABI ã‚µãƒãƒ¼ãƒˆ

* `armeabi-v7a` â€“ Detects `vfpv4`
* `arm64-v8a` â€“ Detects `fp16`
* `x86` / `x86_64` also supported

Runtime detection ensures optimal native performance.
å®Ÿè¡Œæ™‚ã« ABI ã«å¿œã˜ãŸæœ€é©ãªãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’é¸æŠã—ã¾ã™ã€‚

---

## ğŸ”’ License / ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

* This wrapper: Apache 2.0
* llama.cpp: MIT License

---

## ğŸ™ Credits / è¬è¾

* [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)
* Android NDK, Jetpack Compose, Kotlin team
* Optimized and maintained by [Ishizuki Tech LLC](https://ishizuki.tech)

---

## ğŸ“¬ Contact / ãŠå•ã„åˆã‚ã›

Developed by **Ishizuki Tech LLC**
Email: [ishizuki.tech@gmail.com](mailto:ishizuki.tech@gmail.com)
Built for offline AI in the real world.
