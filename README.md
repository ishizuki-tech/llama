# ğŸ¦™ Llama.cpp Android Integration | Android å‘ã‘ Llama.cpp çµ±åˆ

A minimal, production-ready integration of [llama.cpp](https://github.com/ggerganov/llama.cpp) on Android using JNI and Jetpack Compose.

Jetpack Compose ã¨ JNI ã‚’ç”¨ã„ã¦ã€[llama.cpp](https://github.com/ggerganov/llama.cpp) ã‚’ Android ã«çµ±åˆã™ã‚‹æœ€å°é™ã‹ã¤å®Ÿç”¨çš„ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã™ã€‚

---

## âœ… Features / ä¸»ãªç‰¹å¾´

- Native model inference on-device (fully offline)  
  ãƒ¢ãƒ‡ãƒ«ã‚’ç«¯æœ«ä¸Šã§ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œï¼ˆå®Œå…¨ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å‹•ä½œï¼‰

- Built with NDK + CMake (`libllama.so`)  
  NDK + CMake ã«ã‚ˆã‚‹ C++ ãƒ“ãƒ«ãƒ‰ï¼ˆ`libllama.so`ï¼‰

- Kotlin-side wrapper with coroutine support  
  Kotlin ã‹ã‚‰ã‚³ãƒ«ãƒ¼ãƒãƒ³ã§å®‰å…¨ã«å‘¼ã³å‡ºã—

- Automatic CPU threading optimization  
  CPU æ§‹æˆã«å¿œã˜ãŸã‚¹ãƒ¬ãƒƒãƒ‰æœ€é©åŒ–

---

## ğŸš€ Quick Start / ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. Clone the repository / ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³

```bash
git clone git@github.com:ishizuki-tech/llama.git
cd llama
````

### 2. Add model to assets / ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ 

Place a quantized `.gguf` model (e.g., `Llama-3.2-1B-Instruct-Q4_K_M.gguf`) into:

```
app/src/main/assets/models/
```

> âš ï¸ GitHub ã§ã¯ 100MB è¶…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥ push ã§ãã¾ã›ã‚“ã€‚
> Use [Git LFS](https://git-lfs.github.com) or manage manually.
> Git LFS ã¾ãŸã¯æ‰‹å‹•ã§ç®¡ç†ã—ã¦ãã ã•ã„ã€‚

### 3. Open in Android Studio / Android Studio ã§é–‹ã

* Compile SDK: 35+
* NDK: 26.3+
* Run on device with **2GB+ RAM**
  ç«¯æœ«ã¯ RAM 2GB ä»¥ä¸Šæ¨å¥¨

---

## ğŸ“‚ Project Structure / ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ

```
.
â”œâ”€â”€ app/                        # Jetpack Compose UI
â”‚   â””â”€â”€ src/main/assets/models/ # ãƒ¢ãƒ‡ãƒ«æ ¼ç´ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
â”œâ”€â”€ nativelib/                 # JNI & ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚³ãƒ¼ãƒ‰
â”‚   â”œâ”€â”€ src/main/cpp/llama.cpp # llama.cpp ã‚½ãƒ¼ã‚¹
â”‚   â””â”€â”€ src/main/java/com/negi/nativeLib/LlamaContext.kt        # JNI ãƒ©ãƒƒãƒ‘ãƒ¼ï¼ˆKotlinï¼‰
```

---

## ğŸ§  Sample Kotlin Usage / Kotlin åˆ©ç”¨ä¾‹

```kotlin
val llama = LlamaContext.init(context.assets, "models/Llama-3.2-1B-Instruct-Q4_K_M.gguf")

val response = llama.complete("What is the capital of Kenya?")

println(response) // => The capital of Kenya is Nairobi.
```

---

## ğŸ“˜ Model Details: `Llama-3.2-1B-Instruct-Q4_K_M.gguf`

### ğŸ‡¬ğŸ‡§ English

`Llama-3.2-1B-Instruct-Q4_K_M.gguf` is a quantized version of Meta's LLaMA 3.2 model, specially tuned for instruction-following tasks. Itâ€™s optimized for on-device use and compatible with `llama.cpp`.

| Attribute        | Description                            |
| ---------------- | -------------------------------------- |
| Model            | LLaMA 3.2 (Meta)                       |
| Size             | \~770MB                                |
| Parameters       | 1 Billion (1B)                         |
| Format           | `.gguf` (next-gen GGML format)         |
| Tuning           | Instruct (single-turn prompt-response) |
| Quantization     | Q4\_K\_M (4-bit, memory-efficient)     |
| Best for         | Mobile / offline apps                  |
| Language Support | English (primary), basic multilingual  |

**Advantages:**

* Lightweight enough for mobile inference
* Fast response with minimal memory usage
* Suitable for offline prompt completion

**Notes:**

* Do not commit this model to GitHub directly (use Git LFS or manual download)
* You can find this model at: [Hugging Face](https://huggingface.co/TheBloke/LLaMA-3.2-1B-Instruct-GGUF)

---

### ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª

`Llama-3.2-1B-Instruct-Q4_K_M.gguf` ã¯ã€Meta ç¤¾ã® LLaMA 3.2 ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸã€**å‘½ä»¤å¿œç­”ï¼ˆInstructï¼‰å‘ã‘ã®é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«**ã§ã™ã€‚è»½é‡ã‹ã¤ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å‹•ä½œã«æœ€é©åŒ–ã•ã‚Œã¦ãŠã‚Šã€`llama.cpp` ã§ã®åˆ©ç”¨ã‚’å‰æã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚

| é …ç›®     | å†…å®¹                       |
| ------ | ------------------------ |
| ãƒ¢ãƒ‡ãƒ«    | LLaMA 3.2ï¼ˆMetaï¼‰          |
| ã‚µã‚¤ã‚º    | ç´„770MB                   |
| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | ç´„10å„„ï¼ˆ1Bï¼‰                 |
| ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ | `.gguf`ï¼ˆæ¬¡ä¸–ä»£ GGML ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰ |
| ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° | Instructï¼ˆå‘½ä»¤å¿œç­”ï¼‰           |
| é‡å­åŒ–æ–¹å¼  | Q4\_K\_Mï¼ˆ4ãƒ“ãƒƒãƒˆé‡å­åŒ–ãƒ»çœãƒ¡ãƒ¢ãƒªï¼‰   |
| æ¨å¥¨ç”¨é€”   | ãƒ¢ãƒã‚¤ãƒ« / ã‚ªãƒ•ãƒ©ã‚¤ãƒ³åˆ©ç”¨           |
| è¨€èªå¯¾å¿œ   | ä¸»ã«è‹±èªã€ä¸€éƒ¨å¤šè¨€èªå¯¾å¿œã‚ã‚Š           |

**ä¸»ãªåˆ©ç‚¹:**

* ãƒ¢ãƒã‚¤ãƒ«ã§ã‚‚å®Ÿè¡Œã§ãã‚‹è»½é‡ã‚µã‚¤ã‚º
* ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ãŒå°‘ãªãã€æ¨è«–ãŒé«˜é€Ÿ
* å˜ä¸€ã‚¿ãƒ¼ãƒ³ã®æ–‡ç« ç”Ÿæˆã«é©ã—ã¦ã„ã‚‹

**æ³¨æ„ç‚¹:**

* GitHub ã«ç›´æ¥ã‚³ãƒŸãƒƒãƒˆã—ãªã„ã§ãã ã•ã„ï¼ˆGit LFS ã¾ãŸã¯æ‰‹å‹•é…ç½®ã‚’æ¨å¥¨ï¼‰
* ãƒ¢ãƒ‡ãƒ«é…å¸ƒå…ƒï¼š[Hugging Face](https://huggingface.co/TheBloke/LLaMA-3.2-1B-Instruct-GGUF)

---

## ğŸ“¦ Model Suggestions / æ¨å¥¨ãƒ¢ãƒ‡ãƒ«

| Model                 | Size    | Min RAM | Notes                    |
| --------------------- | ------- | ------- | ------------------------ |
| LLaMA-3.2 1B Q4\_K\_M | \~770MB | \~2GB   | Good baseline for mobile |
| LLaMA-3.2 3B Q4\_K\_M | \~2.7GB | \~5GB+  | High-end Android only    |

---

## ğŸ”§ ABI Support / ABI ã‚µãƒãƒ¼ãƒˆ

* `arm64-v8a` â€“ Detects `fp16`

Runtime detection ensures optimal native performance.
å®Ÿè¡Œæ™‚ã« ABI ã«å¿œã˜ãŸæœ€é©ãªãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’é¸æŠã—ã¾ã™ã€‚

---

## ğŸ”’ License / ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

* This wrapper: MIT License
* llama.cpp: MIT License

---

## ğŸ™ Credits / è¬è¾

* [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)
* Android NDK, Jetpack Compose, Kotlin team
* Optimized and maintained by [Ishizuki Tech LLC](https://ishizuki.tech)

---

## ğŸ“¬ Contact / ãŠå•ã„åˆã‚ã›

Developed by **Ishizuki Tech LLC**
Email: [ishizuki.tech@gmail.com](mailto:ishizuki.tech@gmail.com)
